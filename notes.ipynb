{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não esquecer de colocar um detetor de data-drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features `s3BucketLogging` `Dockerizing` `E-mail`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-step to reproducitibity\n",
    "\n",
    "Create a .ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "ClassConfiguration\n",
    "\n",
    "ClassArtifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow in DagsHub, and DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train our model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conda Environment with python `3.10.15`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this project is on the code, to build an automated CICD pipeline in industry level code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Structure Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `S3 bucket` for saving the loggings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante a criação do training pode passar um classe de `Params`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP - Minimal Viable Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we aim to build a  minimal model capable of solving the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `.\\mvp-notebook`: \n",
    "\n",
    "* `1-mvp-development.ipynb` - exploratory data analysys, for data quality assurancem data understanding; model pipeline (preprocessing, training) creation, and evaluation.\n",
    "\n",
    "Please, make sure to check this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the best model is `r2_score of 88,31%` and `mean absolute error of 4,14`, which are great results, considering that our target variable, varies from `9-100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Versioning With DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizes images, audio, files, video and text in **storage** and organize your ml modeling into a **reproductible** workflow\n",
    "\n",
    "**✨ Basic Uses of DVC**\n",
    "\n",
    "If you store and process data files 📂 or datasets 📊 to produce other data or machine learning models 🤖, and you want to:\n",
    "\n",
    "📝 Track and save data and ML models just like you do with code;\n",
    "\n",
    "🔄 Create and switch between **versions of data** and ML models easily;\n",
    "\n",
    "🔍 Understand how datasets and ML artifacts were built in the first place;\n",
    "\n",
    "📊 Compare model metrics among experiments;\n",
    "\n",
    "🛠️ Adopt engineering tools and best practices in data science projects;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to focus on the Data Versioning Functionalities, later we will, use the DVC to improve our pipeline (make able to restart the pipeline from which step the code stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upar o Logging no S3Bucket, ou no volume do ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montar o .ENV passando variáveis de ambientes, ou montando no volume, jamais upar o .env na dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "- name: Run Docker Image to serve users\n",
    "  run: |\n",
    "    echo \"Creating .env file from GitHub Secrets\"\n",
    "    echo \"AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}\" >> .env\n",
    "    echo \"AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}\" >> .env\n",
    "    echo \"AWS_REGION=${{ secrets.AWS_REGION }}\" >> .env\n",
    "    echo \"OTHER_ENV_VAR=${{ secrets.OTHER_ENV_VAR }}\" >> .env\n",
    "\n",
    "    docker run -d -p 8080:8080 \\\n",
    "      --name cnncls \\\n",
    "      -v $(pwd)/.env:/app/.env \\\n",
    "      ${{secrets.AWS_ECR_LOGIN_URI}}/${{ secrets.ECR_REPOSITORY_NAME }}:latest\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=\"/app/.env\")\n",
    "\n",
    "# Access environment variables\n",
    "aws_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "print(f\"Using AWS Region: {region}\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differents Approachs Reusing the Same File Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The aim of this project, was to make a fully automated, data-centric project however with little adjustements this project could support human interation, such as:\n",
    "\n",
    "* Train different Models, report their performance, and asks to the user which one of them he want to tune, as well with the Grid Search Parameters;\n",
    "\n",
    "* Perform the process above, and ask what models the user wants to ensemble;\n",
    "\n",
    "* These process could run automatically, and them notify via e-mail(or a dashboard) when the user needs to interact;\n",
    "\n",
    "* The inputs could be done by `.txt`, `.csv`, `.yml`, or even the standart python input function. The DVC functionality, kinds of \"cache\" the process so we can return from which step we end, without losing much time; and\n",
    "\n",
    "* If that is our objective, we can implement and iterative human-in-the-loop Error-Analysys/Feature Engineering.\n",
    "\n",
    "By doing that, we will have a process with less automation, but with greater performance and more human control\n",
    "\n",
    "By now this code is triggered by pull/push, however it also can be triggered by different options, and be classified as **`Continous Training(CT)`**(for now the re-training is triggered by the user) with a few adjustements:\n",
    "\n",
    "* Locally (cost saving) we could use the windows `task_scheduler` to run a `.py` file that checks for changes in our dataset(number of samples as an example) time by time(every 4 hours as an example), and then triggers our pipeline once some condition gets satisfied.\n",
    "\n",
    "* Cloud, we could use some service, such as `AWS lambda`, uncomment the endpoint of `/train` (commented for deployment), and them trigger this endpoint when some condition gets satisffied.\n",
    "\n",
    "Maintenance\n",
    "\n",
    "* This code is basically maintaned, by change the `@dataclass` in `componentent`, the methods of this class, and then the pipelines. However, some people might prefer to create a `\\.config` folder, and them put some files: [`config.yml`, `params.yml`, `schemas.yml`], that way the user made the changes in those files. I prefer to use directly the python file, because it is cleaner, more readable, and very easy to mantain, and simpler to add new functionalities.\n",
    "\n",
    "\n",
    "Futuer improvements, implement Neural networks that deals with tabular data, and RapidsAI, in case someones got a GPU (performance gains)\n",
    "\n",
    "The project could use an automatically ensemble part\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-cicd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
