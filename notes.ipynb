{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N√£o esquecer de colocar um detetor de data-drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features `s3BucketLogging` `Dockerizing` `E-mail`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar Tkinter\n",
    "\n",
    "Implementar Batchs Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-step to reproducitibity\n",
    "\n",
    "Create a .ENV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@dataclass\n",
    "ClassConfiguration\n",
    "\n",
    "ClassArtifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow in DagsHub, and DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train our model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conda Environment with python `3.10.15`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this project is on the code, to build an automated CICD pipeline in industry level code\n",
    "\n",
    "Usar tkinter para melhorar a estrutura do input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Structure Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `S3 bucket` for saving the loggings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante a cria√ß√£o do training pode passar um classe de `Params`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVP - Minimal Viable Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we aim to build a  minimal model capable of solving the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `.\\mvp-notebook`: \n",
    "\n",
    "* `1-mvp-development.ipynb` - exploratory data analysys, for data quality assurancem data understanding; model pipeline (preprocessing, training) creation, and evaluation.\n",
    "\n",
    "Please, make sure to check this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the best model is `r2_score of 88,31%` and `mean absolute error of 4,14`, which are great results, considering that our target variable, varies from `9-100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Versioning With DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizes images, audio, files, video and text in **storage** and organize your ml modeling into a **reproductible** workflow\n",
    "\n",
    "**‚ú® Basic Uses of DVC**\n",
    "\n",
    "If you store and process data files üìÇ or datasets üìä to produce other data or machine learning models ü§ñ, and you want to:\n",
    "\n",
    "üìù Track and save data and ML models just like you do with code;\n",
    "\n",
    "üîÑ Create and switch between **versions of data** and ML models easily;\n",
    "\n",
    "üîç Understand how datasets and ML artifacts were built in the first place;\n",
    "\n",
    "üìä Compare model metrics among experiments;\n",
    "\n",
    "üõ†Ô∏è Adopt engineering tools and best practices in data science projects;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to focus on the Data Versioning Functionalities, later we will, use the DVC to improve our pipeline (make able to restart the pipeline from which step the code stopped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                     |\n",
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n",
      "|     Read the analytics documentation (and how to opt-out) here:     |\n",
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n",
      "|                                                                     |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "What's next?\n",
      "------------\n",
      "- Check out the documentation: <https://dvc.org/doc>\n",
      "- Get help and share ideas: <https://dvc.org/chat>\n",
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    }
   ],
   "source": [
    "# Initialize DVC in Your Project\n",
    "! dvc init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm 'raw_data/students.csv'\n",
      "[main 977b1c5] Stop tracking raw_data/students.csv with Git\n",
      " 4 files changed, 6 insertions(+), 1001 deletions(-)\n",
      " create mode 100644 .dvc/.gitignore\n",
      " create mode 100644 .dvc/config\n",
      " create mode 100644 .dvcignore\n",
      " delete mode 100644 raw_data/students.csv\n"
     ]
    }
   ],
   "source": [
    "# Stop Git from Tracking students.csv\n",
    "! git rm -r --cached raw_data/students.csv\n",
    "! git commit -m \"Stop tracking raw_data/students.csv with Git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'raw_data\\students.csv.dvc' 'raw_data\\.gitignore'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚†ã Checking graph\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Track the students.csv File\n",
    "! dvc add raw_data/students.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 934b499] Track raw_data/students.csv with DVC\n",
      " 1 file changed, 5 insertions(+)\n",
      " create mode 100644 raw_data/students.csv.dvc\n"
     ]
    }
   ],
   "source": [
    "# Commit Changes to Git\n",
    "! git add raw_data/students.csv.dvc .gitignore\n",
    "! git commit -m \"Track raw_data/students.csv with DVC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the DVC is taking care of our data versioning control.\n",
    "\n",
    "The versions are being cached, and can be restored by as checkpoints by the md5 identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVC common Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the Data Version (Local):\n",
    "\n",
    "1. Update the file\n",
    "\n",
    "2. Update the DVC Tracking\n",
    "\n",
    "```Bash\n",
    "dvc add raw_data/students.csv\n",
    "```\n",
    "\n",
    "3. Commit Changes to Git\n",
    "\n",
    "```Bash\n",
    "git add raw_data/students.csv.dvc\n",
    "git commit -m \"Updated raw_data/students.csv to the new version <insert details of data version here, or control it by a external log>\"\n",
    "```\n",
    "\n",
    "#### Restoring an Older Data Version(Local)\n",
    "\n",
    "1. Identify the Desired Git Commit\n",
    "\n",
    "```Bash\n",
    "git log\n",
    "```\n",
    "\n",
    "Look for the commit message related to the version you want to restore.\n",
    "\n",
    "2. Checkout the Desired Commit\n",
    "\n",
    "```bash\n",
    "git checkout <commit_hash>\n",
    "```\n",
    "\n",
    "3. Checkout the Desired Commit\n",
    "\n",
    "```bash\n",
    "dvc checkout\n",
    "```\n",
    "\n",
    "**This is will probably change your git status to detached HEAD, be sure to return it and to merge all the necessaries changes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further on this project we are going to set a remote repository `DagsHub` to our `DVC` (now, the data versions are within the folder `/.dvc/cache`), and to our `MLFlow Experiment Tracking`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done inside `src/utils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically created the all the files, that we are going to use in our pipelines to:\n",
    "\n",
    "* Send e-mails;\n",
    "* Log; and\n",
    "* Handle Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use e-mails in python you need to create a password App:\n",
    "\n",
    "**Use App Passwords (Recommended for 2FA accounts)**:\n",
    "\n",
    "1. Go to your Google Account settings.\n",
    "2. Navigate to Security, then click on App Passwords.\n",
    "3. Select Mail and Other (Custom name), then generate the password.\n",
    "4. This will give you a 16-character password that you can use in your Python script instead of your Google account password.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities (dataclasses) -> components -> init.py(instantiate) -> pipeline(call methods)\n",
    "\n",
    "In entities, we are going to have the inputs (configurations) and the output(artifacts)\n",
    "\n",
    "Componentes Create the methods\n",
    "\n",
    "In the pipelines, we assemble together our components\n",
    "\n",
    "\n",
    "Componente\n",
    "* Artefatos Anteriores\n",
    "* Artefatos Superiores\n",
    "* Par√¢metrps imputados pelo usu√°rio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar um react para fazer as configura√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valida√ß√£o do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`schemas/`\n",
    "\n",
    "`   reference_stats.json`\n",
    "\n",
    "`   data_structure.py`\n",
    "\n",
    "`research/`\n",
    "\n",
    "`   data_validation.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Data Format with `Pandera` and `validate data drift`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically created a tool for validating that our input share critical properties with the data in which we developed the preprocessing steps, and trained our model (this aims to avoid a lot of headache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks for data depreciation (differences on the distributions of the dataframe we train and the new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sempre rodar tendo por base o root folder, desta forma n√£o haver√° problema de import\n",
    "(mlops-cicd) C:\\Users\\Marina\\Desktop\\cicd-project>python -m src.pipelines.training_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upar o Logging no S3Bucket, ou no volume do ECR, salvar os NP arrays das predi√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montar o .ENV passando vari√°veis de ambientes, ou montando no volume, jamais upar o .env na dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "- name: Run Docker Image to serve users\n",
    "  run: |\n",
    "    echo \"Creating .env file from GitHub Secrets\"\n",
    "    echo \"AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}\" >> .env\n",
    "    echo \"AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}\" >> .env\n",
    "    echo \"AWS_REGION=${{ secrets.AWS_REGION }}\" >> .env\n",
    "    echo \"OTHER_ENV_VAR=${{ secrets.OTHER_ENV_VAR }}\" >> .env\n",
    "\n",
    "    docker run -d -p 8080:8080 \\\n",
    "      --name cnncls \\\n",
    "      -v $(pwd)/.env:/app/.env \\\n",
    "      ${{secrets.AWS_ECR_LOGIN_URI}}/${{ secrets.ECR_REPOSITORY_NAME }}:latest\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=\"/app/.env\")\n",
    "\n",
    "# Access environment variables\n",
    "aws_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "print(f\"Using AWS Region: {region}\")\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differents Approachs Reusing the Same File Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The aim of this project, was to make a fully automated, data-centric project however with little adjustements this project could support human interation, such as:\n",
    "\n",
    "* Train different Models, report their performance, and asks to the user which one of them he want to tune, as well with the Grid Search Parameters;\n",
    "\n",
    "* Perform the process above, and ask what models the user wants to ensemble;\n",
    "\n",
    "* These process could run automatically, and them notify via e-mail(or a dashboard) when the user needs to interact;\n",
    "\n",
    "* The inputs could be done by `.txt`, `.csv`, `.yml`, or even the standart python input function. The DVC functionality, kinds of \"cache\" the process so we can return from which step we end, without losing much time; and\n",
    "\n",
    "* If that is our objective, we can implement and iterative human-in-the-loop Error-Analysys/Feature Engineering.\n",
    "\n",
    "* This was not strictaly necessary in this project, but if in a future application we want to support use interactions, we could, accept `params.yaml` and `config.yaml` inputs, validate them throght `yamale` or `cerberus`,read throught the already existing utils read_yaml function  \n",
    "\n",
    "\n",
    "By doing that, we will have a process with less automation, but with greater performance and more human control\n",
    "\n",
    "By now this code is triggered by pull/push, however it also can be triggered by different options, and be classified as **`Continous Training(CT)`**(for now the re-training is triggered by the user) with a few adjustements:\n",
    "\n",
    "* Locally (cost saving) we could use the windows `task_scheduler` to run a `.py` file that checks for changes in our dataset(number of samples as an example) time by time(every 4 hours as an example), and then triggers our pipeline once some condition gets satisfied.\n",
    "\n",
    "* Cloud, we could use some service, such as `AWS lambda`, uncomment the endpoint of `/train` (commented for deployment), and them trigger this endpoint when some condition gets satisffied.\n",
    "\n",
    "Maintenance\n",
    "\n",
    "* This code is basically maintaned, by change the `@dataclass` in `componentent`, the methods of this class, and then the pipelines. However, some people might prefer to create a `\\.config` folder, and them put some files: [`config.yml`, `params.yml`, `schemas.yml`], that way the user made the changes in those files. I prefer to use directly the python file, because it is cleaner, more readable, and very easy to mantain, and simpler to add new functionalities.\n",
    "\n",
    "\n",
    "Futuer improvements, implement Neural networks that deals with tabular data, and RapidsAI, in case someones got a GPU (performance gains)\n",
    "\n",
    "The project could use an automatically ensemble part\n",
    "\n",
    "Dinamically handle different file extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Mainentance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new dataframe\n",
    "\n",
    "1.  Update the reference stats\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Example reference data\n",
    "reference_data = pd.read_csv(\"raw_data/students.csv\")\n",
    "\n",
    "# Compute statistics\n",
    "reference_stats = {}\n",
    "\n",
    "# Numerical features\n",
    "for feature in numeric_features:\n",
    "    reference_stats[feature] = {\n",
    "        \"mean\": reference_data[feature].mean(),\n",
    "        \"std\": reference_data[feature].std(),\n",
    "        \"percentiles\": reference_data[feature].quantile([0.25, 0.5, 0.75]).to_dict()\n",
    "    }\n",
    "\n",
    "# Categorical features\n",
    "for feature in categorical_features:\n",
    "    reference_stats[feature] = {\n",
    "        \"value_counts\": reference_data[feature].value_counts(normalize=True).to_dict(),\n",
    "    }\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"schemas/reference_stats.json\", \"w\") as f:\n",
    "    json.dump(reference_stats, f)\n",
    "```\n",
    "\n",
    "2. Update the Pandera Class\n",
    "\n",
    "For new Steps/components:\n",
    "\n",
    "1. Update The Entities.py with the Input/Artifact/Configuration class for the step\n",
    "\n",
    "2. Create a new component file, create a class importing the @dataclasses of the previous step. Creathe the methods\n",
    "\n",
    "3. Update the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-cicd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
